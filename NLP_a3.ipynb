{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGS 20006/30006 : NLP Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen 'The Count of Monte Cristo' By Alexandre Dumas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"MC.txt\") as f:\n",
    "    content = f.read().splitlines()\n",
    "    #content = list(filter(None, content))\n",
    "    content = \" \".join(content)\n",
    "    content = content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter 1. marseilles—the arrival  on the 24th of february, 1815, the look-out at notre-dame de la garde signalled the three-master, the pharaon from smyrna, trieste, and naples.  as usual, a pilot put off immediately, and rounding the château d’if, got on board the vessel between cape morgiou and rion island.  immediately, and according to custom, the ramparts of fort saint-jean were covered with spectators; it is always an event at marseilles for a ship to come into port, especially when this ship, like the pharaon, has been built, rigged, and laden at the old phocee docks, and belongs to an owner of the city.  the ship drew on and had safely passed the strait, which some volcanic shock has made between the calasareigne and jaros islands; had doubled pomègue, and approached the harbor under topsails, jib, and spanker, but so slowly and sedately that the idlers, with that instinct which is the forerunner of evil, asked one another what misfortune could have happened on board. however,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw length of my text is 2617253\n"
     ]
    }
   ],
   "source": [
    "print(\"The raw length of my text is %d\" % len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use NLTK & SpaCy to tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73 s ± 82.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "['chapter', '1.', 'marseilles—the', 'arrival', 'on', 'the', '24th', 'of', 'february', ',', '1815', ',', 'the', 'look-out', 'at', 'notre-dame', 'de', 'la', 'garde', 'signalled', 'the', 'three-master', ',', 'the', 'pharaon', 'from', 'smyrna', ',', 'trieste', ',', 'and', 'naples', '.', 'as', 'usual', ',', 'a', 'pilot', 'put', 'off', 'immediately', ',', 'and', 'rounding', 'the', 'château', 'd', '’', 'if', ',', 'got', 'on', 'board', 'the', 'vessel', 'between', 'cape', 'morgiou', 'and', 'rion', 'island', '.', 'immediately', ',', 'and', 'according', 'to', 'custom', ',', 'the', 'ramparts', 'of', 'fort', 'saint-jean', 'were', 'covered', 'with', 'spectators', ';', 'it', 'is', 'always', 'an', 'event', 'at', 'marseilles', 'for', 'a', 'ship', 'to', 'come', 'into', 'port', ',', 'especially', 'when', 'this', 'ship', ',', 'like']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = nltk.word_tokenize(content)\n",
    "Nw = %timeit -o nltk.tokenize.word_tokenize(content)\n",
    "print(nltk_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34 s ± 32.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "spacy_tokens = tokenizer(content)\n",
    "Sw = %timeit -o tokenizer(content)\n",
    "stokens = []\n",
    "for token in spacy_tokens:\n",
    "    stokens.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of NLTK tokens is 570102\n",
      "The unique number of NLTK tokens is 20925\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of NLTK tokens is %d\" % len(nltk_tokens))\n",
    "print(\"The unique number of NLTK tokens is %d\" % len(set(nltk_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of SpaCy tokens is 474926\n",
      "The unique number of SpaCy tokens is 38082\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of SpaCy tokens is %d\" % len(stokens))\n",
    "print(\"The unique number of SpaCy tokens is %d\" % len(set(stokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_token(a, b): \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "    return (a_set & b_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = common_token(nltk_tokens, stokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of common tokens in both is 16731\n",
      "The number of unique tokens in NLTK is 21351\n",
      "The number of unique tokens in SpaCy is 4194\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of common tokens in both is %d\" % len(common))\n",
    "print(\"The number of unique tokens in NLTK is %d\" % (len(set(stokens))-len(common)))\n",
    "print(\"The number of unique tokens in SpaCy is %d\" % (len(set(nltk_tokens))-len(common)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"fed age alling added bing cleaned'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alling', 'added']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\ba[A-Za-z]*ing\\b|\\ba[A-Za-z]*ed\\b]*\\b\", test)\n",
    "#re.findall(r'\\ba\\w*(?:ing|ed)\\b', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. All words beginning with a– and ending in –ing or –ed.\n",
    "inged = set(re.findall(r\"\\ba[A-Za-z]*ing\\b|\\ba[A-Za-z]*ed\\b]*\\b\", content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attaching', 'accused', 'accosting', 'advertised', 'annoyed', 'alleging', 'alluding', 'amusing', 'amused', 'atoning', 'attracting', 'avowed', 'afforded', 'attuned', 'advised', 'alluded', 'abandoning', 'apologizing', 'aching', 'assassinated', 'adapted', 'accustoming', 'attached', 'anticipating', 'attained', 'accursed', 'accepting', 'appropriated', 'appalled', 'approving', 'assumed', 'arrived', 'asking', 'awarded', 'approaching', 'accustomed', 'acquitted', 'approached', 'applying', 'apologized', 'annulled', 'anchoring', 'attributing', 'addressing', 'alienated', 'attracted', 'anticipated', 'appointed', 'awning', 'accompanied', 'ascertained', 'analyzed', 'altered', 'afflicted', 'arming', 'applauding', 'attempting', 'anointed', 'accosted', 'ashamed', 'allying', 'administering', 'awakened', 'alluring', 'absorbing', 'appeared', 'appearing', 'awed', 'anchored', 'associated', 'astounded', 'aiding', 'aimed', 'arriving', 'adjoining', 'avenging', 'assailed', 'amazed', 'adding', 'advanced', 'adjusting', 'adjoined', 'attacked', 'acknowledging', 'assuming', 'affianced', 'absorbed', 'associating', 'ascending', 'amounted', 'assigned', 'awaked', 'abased', 'ached', 'apprehended', 'allied', 'admiring', 'attested', 'abdicated', 'aided', 'allotted', 'accounting', 'amounting', 'appealed', 'aggravated', 'accounted', 'accomplished', 'apprehending', 'asserted', 'arranging', 'affixed', 'alighted', 'adhering', 'amazing', 'annoying', 'assisting', 'appreciated', 'authorized', 'articulating', 'assuring', 'astonished', 'ascended', 'attired', 'armed', 'aroused', 'adopted', 'awaited', 'affecting', 'arrested', 'agitated', 'announced', 'astonishing', 'according', 'attended', 'adopting', 'agreed', 'acted', 'accusing', 'avoiding', 'arrayed', 'alarmed', 'adjusted', 'acknowledged', 'arched', 'affording', 'astounding', 'appeased', 'aspiring', 'asserting', 'acceded', 'achieved', 'apprised', 'agonized', 'abandoned', 'advising', 'accepted', 'avenged', 'admired', 'administered', 'antiquated', 'accelerating', 'admitted', 'arranged', 'augmented', 'annotating', 'availing', 'amassed', 'abetted', 'accomplishing', 'assigning', 'appealing', 'adorned', 'attributed', 'assured', 'allowing', 'acquired', 'ambled', 'affected', 'advancing', 'applied', 'absolved', 'asked', 'admitting', 'approved', 'announcing', 'appalling', 'ascertaining', 'acquainted', 'abused', 'affirmed', 'argued', 'arousing', 'accompanying', 'added', 'acting', 'adored', 'assisted', 'actuated', 'addressed', 'attacking', 'assented', 'abated', 'abstracted', 'awaking', 'accumulated', 'adjourned', 'advocated', 'abolished', 'appetizing', 'attempted', 'arresting', 'addicted', 'awaiting', 'averting', 'accumulating', 'avoided', 'availed', 'alleviated', 'answered', 'abounded', 'anything', 'arising', 'awakening', 'agonizing', 'accorded', 'animated', 'assembled', 'alternating', 'allowed', 'alighting', 'answering', 'alarming', 'attending', 'aged', 'abjured'}\n"
     ]
    }
   ],
   "source": [
    "print(inged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. All words with –ing– in the middle of the word.\n",
    "pattern = \"ing\"\n",
    "m = set(re.findall(r\"\\b[A-Za-z]+ing[a-z]+\\b\", content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stings', 'unwillingly', 'nothingness', 'buildings', 'willingly', 'distinguishable', 'springing', 'gnawings', 'accordingly', 'ravellings', 'underlings', 'flinging', 'coverings', 'extinguishers', 'revoltingly', 'dingy', 'hesitatingly', 'openings', 'fringe', 'jingling', 'blessings', 'beings', 'forebodings', 'extinguished', 'linguist', 'fingers', 'unresistingly', 'whitings', 'sufferings', 'scrutinizingly', 'infringed', 'stockings', 'extinguish', 'lingered', 'insultingly', 'proceedings', 'charmingly', 'tinged', 'inquiringly', 'surpassingly', 'distinguished', 'confidingly', 'assentingly', 'clinging', 'despairingly', 'unhesitatingly', 'intermingled', 'doubtingly', 'jottings', 'fringed', 'amusingly', 'infringement', 'laughingly', 'springtide', 'untiringly', 'glimmerings', 'strings', 'smilingly', 'surroundings', 'grudgingly', 'nightingales', 'tinge', 'lingers', 'drawings', 'willingness', 'misgivings', 'feelings', 'beatings', 'bringing', 'rings', 'linguistic', 'singularity', 'hinges', 'turnings', 'mouldings', 'singular', 'lingua', 'ringing', 'clings', 'lingering', 'contingency', 'stingy', 'frowningly', 'strikingly', 'carvings', 'workingman', 'gratings', 'becomingly', 'yearnings', 'failings', 'things', 'tremblingly', 'dealings', 'earrings', 'winged', 'springs', 'kingdom', 'tidings', 'unwittingly', 'jingled', 'single', 'kings', 'bearings', 'doings', 'meaningless', 'railings', 'sayings', 'mingle', 'windings', 'meetings', 'evenings', 'sings', 'wings', 'kingdoms', 'cravings', 'grumblingly', 'furnishings', 'distinguishing', 'nightingale', 'singing', 'throbbings', 'singers', 'hangings', 'mingled', 'singularly', 'linger', 'fastenings', 'distinguish', 'extinguishing', 'musings', 'unwillingness', 'paintings', 'mingling', 'moorings', 'exceedingly', 'finger', 'wringing', 'encouragingly', 'flickerings', 'brings', 'singer'}\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. After removing all punctuation, provide a count of all words in your text containing\n",
    "#numbers or punctuation and letters (if any).\n",
    "clean = re.sub(r'[^\\w\\s\\d]',' ', content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw length of my text without punctuation is 2617253\n"
     ]
    }
   ],
   "source": [
    "print(\"The raw length of my text without punctuation is %d\" % len(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_word = re.findall(\"\\w*[\\d@]\\w*\", content)\n",
    "#re.findall('[A-Za-z]+[\\d@]+[\\w@]*|[\\d@]+[A-Za-z]+[\\w@]*', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '24th', '1815', '0023m', '25', '000', '0025m', '0027m', '0029m', '2']\n"
     ]
    }
   ],
   "source": [
    "print(mixed_word[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of all words in my text containing numbers is 1332\n"
     ]
    }
   ],
   "source": [
    "print(\"The count of all words in my text containing numbers is %d\" % len(mixed_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A selection of different “stems” from the 3 NLTK stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'come', 'on', 'board', ',', 'm.', 'morrel', ',', '”', 'said', 'dantès', ',', 'observing', 'the', 'owner', '’', 's', 'impatience', ',', '“', 'here', 'is', 'your', 'supercargo', ',', 'm.', 'danglars', ',', 'coming', 'out', 'of', 'his', 'cabin', ',', 'who', 'will', 'furnish', 'you', 'with', 'every', 'particular', '.', 'as', 'for', 'me', ',', 'i', 'must', 'look', 'after', 'the', 'anchoring', ',', 'and', 'dress', 'the', 'ship', 'in', 'mourning.', '”', 'the', 'owner', 'did', 'not', 'wait', 'for', 'a', 'second', 'invitation', '.', 'he', 'seized', 'a', 'rope', 'which', 'dantès', 'flung', 'to', 'him', ',', 'and', 'with', 'an', 'activity', 'that', 'would', 'have', 'done', 'credit', 'to', 'a', 'sailor', ',', 'climbed', 'up', 'the', 'side', 'of', 'the', 'ship']\n"
     ]
    }
   ],
   "source": [
    "# Raw token\n",
    "print(nltk_tokens[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'come', 'on', 'board', ',', 'm.', 'morrel', ',', '”', 'said', 'dantè', ',', 'observ', 'the', 'owner', '’', 's', 'impati', ',', '“', 'here', 'is', 'your', 'supercargo', ',', 'm.', 'danglar', ',', 'come', 'out', 'of', 'hi', 'cabin', ',', 'who', 'will', 'furnish', 'you', 'with', 'everi', 'particular', '.', 'as', 'for', 'me', ',', 'i', 'must', 'look', 'after', 'the', 'anchor', ',', 'and', 'dress', 'the', 'ship', 'in', 'mourning.', '”', 'the', 'owner', 'did', 'not', 'wait', 'for', 'a', 'second', 'invit', '.', 'he', 'seiz', 'a', 'rope', 'which', 'dantè', 'flung', 'to', 'him', ',', 'and', 'with', 'an', 'activ', 'that', 'would', 'have', 'done', 'credit', 'to', 'a', 'sailor', ',', 'climb', 'up', 'the', 'side', 'of', 'the', 'ship']\n"
     ]
    }
   ],
   "source": [
    "# Porter\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print([ps.stem(word) for word in nltk_tokens[1000:1100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wil', 'com', 'on', 'board', ',', 'm.', 'morrel', ',', '”', 'said', 'dantè', ',', 'observ', 'the', 'own', '’', 's', 'impaty', ',', '“', 'her', 'is', 'yo', 'supercargo', ',', 'm.', 'dangl', ',', 'com', 'out', 'of', 'his', 'cabin', ',', 'who', 'wil', 'furn', 'you', 'with', 'every', 'particul', '.', 'as', 'for', 'me', ',', 'i', 'must', 'look', 'aft', 'the', 'anch', ',', 'and', 'dress', 'the', 'ship', 'in', 'mourning.', '”', 'the', 'own', 'did', 'not', 'wait', 'for', 'a', 'second', 'invit', '.', 'he', 'seiz', 'a', 'rop', 'which', 'dantè', 'flung', 'to', 'him', ',', 'and', 'with', 'an', 'act', 'that', 'would', 'hav', 'don', 'credit', 'to', 'a', 'sail', ',', 'climb', 'up', 'the', 'sid', 'of', 'the', 'ship']\n"
     ]
    }
   ],
   "source": [
    "# Lancaster\n",
    "from nltk.stem import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "print([lc.stem(word) for word in nltk_tokens[1000:1100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'come', 'on', 'board', ',', 'm.', 'morrel', ',', '”', 'said', 'dantè', ',', 'observ', 'the', 'owner', '’', 's', 'impati', ',', '“', 'here', 'is', 'your', 'supercargo', ',', 'm.', 'danglar', ',', 'come', 'out', 'of', 'his', 'cabin', ',', 'who', 'will', 'furnish', 'you', 'with', 'everi', 'particular', '.', 'as', 'for', 'me', ',', 'i', 'must', 'look', 'after', 'the', 'anchor', ',', 'and', 'dress', 'the', 'ship', 'in', 'mourning.', '”', 'the', 'owner', 'did', 'not', 'wait', 'for', 'a', 'second', 'invit', '.', 'he', 'seiz', 'a', 'rope', 'which', 'dantè', 'flung', 'to', 'him', ',', 'and', 'with', 'an', 'activ', 'that', 'would', 'have', 'done', 'credit', 'to', 'a', 'sailor', ',', 'climb', 'up', 'the', 'side', 'of', 'the', 'ship']\n"
     ]
    }
   ],
   "source": [
    "print([snowball.stem(word) for word in nltk_tokens[1000:1100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique Lemmas(NLTK) is 19095\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "nltk_lemmas = [wl.lemmatize(word) for word in nltk_tokens]\n",
    "print(\"The unique Lemmas(NLTK) is %d\" % len(set(nltk_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique Lemmas(SpaCy) is 33871\n"
     ]
    }
   ],
   "source": [
    "# SpaCy\n",
    "spacy_lemmas = [token.lemma_ for token in spacy_tokens]\n",
    "print(\"The unique Lemmas(SpaCy) is %d\" % len(set(spacy_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1.', 'marseilles—the', 'arrival', 'on', 'the', '24th', 'of', 'february', ',', '1815', ',', 'the', 'look-out', 'at', 'notre-dame', 'de', 'la', 'garde', 'signalled', 'the', 'three-master', ',', 'the', 'pharaon', 'from', 'smyrna', ',', 'trieste', ',', 'and', 'naples', '.', 'as', 'usual', ',', 'a', 'pilot', 'put', 'off', 'immediately', ',', 'and', 'rounding', 'the', 'château', 'd', '’', 'if', ',', 'got', 'on', 'board', 'the', 'vessel', 'between', 'cape', 'morgiou', 'and', 'rion', 'island', '.', 'immediately', ',', 'and', 'according', 'to', 'custom', ',', 'the', 'ramparts', 'of', 'fort', 'saint-jean', 'were', 'covered', 'with', 'spectators', ';', 'it', 'is', 'always', 'an', 'event', 'at', 'marseilles', 'for', 'a', 'ship', 'to', 'come', 'into', 'port', ',', 'especially', 'when', 'this', 'ship', ',', 'like']\n",
      "\n",
      "['chapter', '1.', 'marseilles—the', 'arrival', 'on', 'the', '24th', 'of', 'february', ',', '1815', ',', 'the', 'look-out', 'at', 'notre-dame', 'de', 'la', 'garde', 'signalled', 'the', 'three-master', ',', 'the', 'pharaon', 'from', 'smyrna', ',', 'trieste', ',', 'and', 'naples', '.', 'a', 'usual', ',', 'a', 'pilot', 'put', 'off', 'immediately', ',', 'and', 'rounding', 'the', 'château', 'd', '’', 'if', ',', 'got', 'on', 'board', 'the', 'vessel', 'between', 'cape', 'morgiou', 'and', 'rion', 'island', '.', 'immediately', ',', 'and', 'according', 'to', 'custom', ',', 'the', 'rampart', 'of', 'fort', 'saint-jean', 'were', 'covered', 'with', 'spectator', ';', 'it', 'is', 'always', 'an', 'event', 'at', 'marseille', 'for', 'a', 'ship', 'to', 'come', 'into', 'port', ',', 'especially', 'when', 'this', 'ship', ',', 'like']\n",
      "\n",
      "['chapter', '1.', 'marseilles—the', 'arrival', ' ', 'on', 'the', '24th', 'of', 'february,', '1815,', 'the', 'look-out', 'at', 'notre-dame', 'de', 'la', 'garde', 'signal', 'the', 'three-master,', 'the', 'pharaon', 'from', 'smyrna,', 'trieste,', 'and', 'naples.', ' ', 'a', 'usual,', 'a', 'pilot', 'put', 'off', 'immediately,', 'and', 'round', 'the', 'château', 'd’if,', 'get', 'on', 'board', 'the', 'vessel', 'between', 'cape', 'morgiou', 'and', 'rion', 'island.', ' ', 'immediately,', 'and', 'accord', 'to', 'custom,', 'the', 'rampart', 'of', 'fort', 'saint-jean', 'be', 'cover', 'with', 'spectators;', 'it', 'be', 'always', 'a', 'event', 'at', 'marseilles', 'for', 'a', 'ship', 'to', 'come', 'into', 'port,', 'especially', 'when', 'this', 'ship,', 'like', 'the', 'pharaon,', 'have', 'be', 'built,', 'rigged,', 'and', 'lade', 'at', 'the', 'old', 'phocee', 'docks,', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_tokens[:100])\n",
    "print(\"\")\n",
    "print(nltk_lemmas[:100])\n",
    "print(\"\")\n",
    "print(spacy_lemmas[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same text, using NLTK lemmatizer ended up getting a smaller unique number of lemmas than SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides 9 different stemming libraries, allow users to finely customize their model. But this can be a hindrance. Which algorithm performs the best? Which is the fastest? Which is being maintained?\n",
    "In contrast, spaCy implements a single stemmer, the one that the spaCy developers feel to be best. I guess sometimes it's easier when I have no other choices.\n",
    "\n",
    "NLTK is a string processing library. All the tools take strings as input and return strings or lists of strings as output. \n",
    "On the other hand, spaCy uses an object-oriented approach. Parsing some text returns an object. Each of these objects has a number of useful attributes and methods. This object-oriented approach lends itself to be more Pythonic than does the string-handling system of NLTK. We can expect spaCy may outperform NLTK in speed when dealing with larger text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.DataFrame(np.array([t.best * 1000 for t in (Nw, Sw)]).reshape((1, 2)),columns=[\"NLTK\", \"spaCy\"]).plot.barh()\n",
    "plt.subplots_adjust(left=0.2)\n",
    "plt.title('Timings (Shorter is better)')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.savefig('timing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
